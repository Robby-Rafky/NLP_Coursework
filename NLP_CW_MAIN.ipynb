{"cells":[{"cell_type":"markdown","source":["# Functions, Modules, Downloads and Package Installations\n","\n"],"metadata":{"id":"foCXKPiloXa6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIgrV_La-3PV"},"outputs":[],"source":["# @title Package Installation\n","%pip install transformers datasets tokenizers nltk seqeval -q\n","%pip install accelerate -U\n","%pip install numpy matplotlib seaborn scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnfKyCP3-3PW","cellView":"form"},"outputs":[],"source":["# @title Imports\n","import datasets\n","import numpy as np\n","import json\n","from transformers import BertTokenizerFast\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification\n","from transformers import pipeline\n","from transformers import TrainingArguments, Trainer\n","from datasets import load_dataset, DatasetDict\n","from datasets.utils.logging import disable_progress_bar\n","import nltk\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from nltk.corpus import stopwords, wordnet\n","import random\n","import re\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# @title Dataset & Corpus Downloads\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","stop_words = set(stopwords.words('english'))\n","\n","dataset = datasets.load_dataset(\"surrey-nlp/PLOD-CW\")\n","\n","tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n","\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n","\n","metric = datasets.load_metric(\"seqeval\")\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\",\n","                                                        num_labels = 4)"],"metadata":{"id":"SCVK4FQH6CEP","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ylCB-JE5-3PX","cellView":"form"},"outputs":[],"source":["# @title Hyperparameters & Configs\n","\n","# DEFINITIONS\n","\n","# List of tags in train/test sets\n","LABEL_LIST = sorted([\"B-O\", \"B-AC\", \"B-LF\", \"I-LF\"])\n","\n","# Dynamically create a 0 - n mapping list for the tags\n","NER_MAPPING = {\n","    label: i for i, label in enumerate(LABEL_LIST)\n","}\n","\n","# Remove the progress bars (removes bloat from outputs).\n","disable_progress_bar()\n","\n","# Configs and their reset functions to reset their values after\n","# experiments.\n","def reset_configs():\n","    global MODEL_CONFIG, PREPROCESSING_CONFIG\n","    MODEL_CONFIG = {\n","        \"learning_rate\": 0.00001,\n","        \"num_epochs\": 3,\n","        \"weight_decay\": 0.01,\n","        \"batch_size\": 16\n","    }\n","\n","    PREPROCESSING_CONFIG = {\n","        \"apply_lemmatization\": False,\n","        \"apply_stemming\": False,\n","        \"apply_random_deletion\": False,\n","        \"apply_lowercasing\": False,\n","        \"apply_url_removal\": False,\n","        \"apply_stop_word_removal\": False\n","    }\n","\n","def reset_augments():\n","    global AUGMENTATION_CONFIG\n","    AUGMENTATION_CONFIG = {\n","        \"apply_synonym_replacement\": False,\n","        \"apply_random_insertion\": False,\n","        \"apply_random_swap\": False\n","    }\n","\n","# Run once so these configs actually exist\n","reset_configs()\n","reset_augments()"]},{"cell_type":"code","source":["# @title Pre-Processing\n","\n","# Handle data preprocessing according to config, can apply multiple different\n","# processes to text in sequence.\n","\n","def preprocess_text(tokens, ner_tags, apply_lemmatization=False,\n","                    apply_stemming=False, apply_random_deletion=False,\n","                    apply_lowercasing=False, apply_url_removal=False,\n","                    apply_stop_word_removal=False):\n","\n","    processed_tokens = []\n","    processed_tags = []\n","\n","    for sentence_tokens, sentence_tags in zip(tokens, ner_tags):\n","        sentence_processed_tokens = []\n","        sentence_processed_tags = []\n","\n","        # Iterate through token/tag pairs in the sequence\n","        for token, tag in zip(sentence_tokens, sentence_tags):\n","\n","            # URL removal using a simple regex\n","            if apply_url_removal:\n","                token = re.sub(r'http\\S+', '', token)\n","\n","            # Lowercasing\n","            if apply_lowercasing:\n","                token = token.lower()\n","\n","            # Lemmatization\n","            if apply_lemmatization:\n","                token = lemmatizer.lemmatize(token)\n","\n","            # Stemming\n","            if apply_stemming:\n","                token = stemmer.stem(token)\n","\n","            # Stop word removal, skips the iteration without appending the\n","            # current token/tag pair to \"delete\" them.\n","            if apply_stop_word_removal and token in stop_words:\n","                continue\n","\n","            # Random deletion, randomly skips iterations with a 7.5% chance.\n","            if apply_random_deletion and random.random() < 0.075:\n","                continue\n","\n","            # Append the processed token/tag pair to form a sentence.\n","            sentence_processed_tokens.append(token)\n","            sentence_processed_tags.append(tag)\n","\n","        # Append the sentence to a list of sequences.\n","        processed_tokens.append(sentence_processed_tokens)\n","        processed_tags.append(sentence_processed_tags)\n","\n","    return processed_tokens, processed_tags\n","\n","\n","# Extracts the token and tags split from the dataset to be passed to the text\n","# processor.\n","\n","def preprocess_dataset(example, **preprocess_settings):\n","    return preprocess_text(example['tokens'],\n","                           example['ner_tags'],\n","                           **preprocess_settings)\n","\n"],"metadata":{"id":"OkdieGhrzx4R","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Data Augmentation\n","\n","def synonym_replacement(token):\n","\n","    # Get all synoyms associated with token.\n","    synsets = wordnet.synsets(token)\n","    if not synsets:\n","\n","        # Return the original token if not found.\n","        return token\n","\n","    # Collect all synonyms from the synsets\n","    synonyms = [lem.name().replace('_', ' ') for\n","                syn in synsets for lem in syn.lemmas()]\n","    return random.choice(synonyms) if synonyms else token\n","\n","def augment_sentence(tokens, ner_tags, apply_synonym_replacement=False,\n","                     apply_random_insertion=False, apply_random_swap=False):\n","\n","    # Copy tokens/tags list to avoid modifying the original.\n","    new_tokens = tokens[:]\n","    new_tags = ner_tags[:]\n","\n","    # Randomly swap two tokens and their corresponding tags.\n","    if apply_random_swap:\n","        idx1, idx2 = random.sample(range(len(new_tokens)), 2)\n","        new_tokens[idx1], new_tokens[idx2] = new_tokens[idx2], new_tokens[idx1]\n","        new_tags[idx1], new_tags[idx2] = new_tags[idx2], new_tags[idx1]\n","\n","    # Replace each token with a synonym if possible.\n","    if apply_synonym_replacement:\n","        new_tokens = [synonym_replacement(token) for token in new_tokens]\n","\n","    # Insert a similar word to a random token right after the original token.\n","    if apply_random_insertion:\n","        idx = random.randint(0, len(new_tokens) - 1)\n","        token, tag = new_tokens[idx], new_tags[idx]\n","        synonyms = [lem.name().replace('_', ' ') for syn\n","                    in wordnet.synsets(token) for lem in syn.lemmas()]\n","        if synonyms:\n","            synonym = random.choice(synonyms)\n","            new_tokens.insert(idx + 1, synonym)\n","            new_tags.insert(idx + 1, tag)\n","\n","    return new_tokens, new_tags\n","\n","def augment_dataset(dataset, **augmentation_config):\n","    augmented_tokens = []\n","    augmented_tags = []\n","    # Apply augmentations to each sequence in the dataset.\n","    for i, seq in enumerate(dataset[\"tokens\"]):\n","        tokens, tags = augment_sentence(dataset[\"tokens\"][i],\n","                                        dataset[\"ner_tags\"][i],\n","                                        **augmentation_config)\n","        augmented_tokens.append(tokens)\n","        augmented_tags.append(tags)\n","    return augmented_tokens, augmented_tags\n","\n"],"metadata":{"cellView":"form","id":"3aCqqL3Y0BS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQJ1f0hV-3PX","cellView":"form"},"outputs":[],"source":["# @title Data Processing Handler\n","\n","# These functions cleanly handle all the relevant calls for augmentation,\n","# preprocessing and mapping of tags, and ensures theyre done in a logical order.\n","\n","def ner_to_numerical(example):\n","\n","    # maps the ner_tags to a numerical representation that the\n","    # transformer can interpret.\n","    numerical_tag_set = []\n","    for i, seq in enumerate(example[\"ner_tags\"]):\n","        numerical_tags = []\n","        for tag in seq:\n","            numerical_tags.append(NER_MAPPING[tag])\n","        numerical_tag_set.append(numerical_tags)\n","    return numerical_tag_set\n","\n","def preprocessing_tasks(example):\n","\n","    # Augment data first, Preprocessing after and finally\n","    # map the ner_tags to numerical.\n","    example[\"tokens\"], example[\"ner_tags\"] = augment_dataset(\n","        example, **AUGMENTATION_CONFIG)\n","    example[\"tokens\"], example[\"ner_tags\"] = preprocess_dataset(\n","        example, **PREPROCESSING_CONFIG)\n","    example[\"ner_tags\"] = ner_to_numerical(example)\n","\n","    return example\n","\n","def preprocessing_test_tasks(example):\n","    global PREPROCESSING_CONFIG\n","\n","    # Only deterministic preprocessing should be applied to the test dataset.\n","    if PREPROCESSING_CONFIG[\"apply_random_deletion\"]:\n","\n","        PREPROCESSING_CONFIG[\"apply_random_deletion\"] = False\n","\n","        example[\"tokens\"], example[\"ner_tags\"] = preprocess_dataset(\n","            example, **PREPROCESSING_CONFIG)\n","\n","        PREPROCESSING_CONFIG = True\n","    else:\n","        example[\"tokens\"], example[\"ner_tags\"] = preprocess_dataset(\n","            example, **PREPROCESSING_CONFIG)\n","\n","    # Processing for the test dataset. We dont augment the data.\n","    example[\"ner_tags\"] = ner_to_numerical(example)\n","\n","    return example\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4E9PljZ-3PY","cellView":"form"},"outputs":[],"source":["# @title Data Tokenizer and Token Padding Handler\n","\n","# Tokenize data and handle the extra tokens added to beginning and end of\n","# sequences.\n","\n","def label_tokenize_alignment(example, label_all_tokens = True):\n","    tokenized_input = tokenizer(example[\"tokens\"],\n","                                truncation= True, is_split_into_words=True)\n","    labels = []\n","\n","    # Process each sequence.\n","    for i, label in enumerate(example[\"ner_tags\"]):\n","\n","        # Get the token indices for the current sequence.\n","        word_ids = tokenized_input.word_ids(batch_index=i)\n","        previous_word_idx = None\n","\n","        label_ids = []\n","\n","        for word_idx in word_ids:\n","            if word_idx is None:\n","\n","                # These labels will ignored during training/evaluation.\n","                # Used to replace the start/end of sequences where the\n","                # tokenizer adds its own tokens.\n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            else:\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","    tokenized_input[\"labels\"] = labels\n","    return tokenized_input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9ngr2qD-3Pa","cellView":"form"},"outputs":[],"source":["# @title Testing, Validation and Evaluation\n","\n","# Metrics to be run during training, evaluation and testing. Can be configured\n","# to return labels to feed into a confusion matrix.\n","\n","def compute_metrics(eval_preds, return_labels = False):\n","\n","    # get logits predictions during evaluation\n","    pred_logits, labels = eval_preds\n","    pred_logits = np.argmax(pred_logits, axis=2)\n","\n","    # We ignore all the [token, tag] pairs where the tag is -100\n","    predictions = [[LABEL_LIST[eval_preds] for (eval_preds, l) in\n","                    zip(prediction, label) if l != -100]\n","                   for prediction, label in zip(pred_logits, labels)\n","    ]\n","    true_labels = [[LABEL_LIST[l] for (eval_preds, l) in\n","                    zip(prediction, label)if l != -100]\n","                   for prediction, label in zip(pred_logits, labels)\n","    ]\n","\n","    # Calculate standard metrics for the experiments using the predicted and\n","    # true labels.\n","    results = metric.compute(predictions=predictions, references=true_labels)\n","\n","    # Flatten the 2D lists for confusion matrix computation\n","    flattened_predictions = [p for sublist in predictions for p in sublist]\n","    flattened_true_labels = [t for sublist in true_labels for t in sublist]\n","\n","    # Where the labels are needed, we return the flattened labels or\n","    # the standard metrics for these experiments.\n","    if return_labels:\n","        return {\n","            \"flattened_true_labels\": flattened_true_labels,\n","            \"flattened_predictions\": flattened_predictions\n","        }\n","    else:\n","        return {\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","        }\n","\n","\n","# Calls processing and augmentation functions on the dataset based on config.\n","# Trains, evaluates and tests a pretrained transformer model to predict\n","# ner_tags. Returns true and predicted labels for confusion matrix generating.\n","\n","def train_validate_evaluate():\n","\n","    # Separate testing portion of the dataset to use map functions on the\n","    # training and validation sets without interfering with the testing set.\n","    test_set = DatasetDict({\n","        \"test\": dataset[\"test\"]\n","    })\n","    train_valid_set = DatasetDict({\n","        \"train\": dataset[\"train\"],\n","        \"validation\": dataset[\"validation\"]\n","    })\n","\n","    # Run preprocessing tasks for the dataset splits according to configs.\n","    processed_dataset = train_valid_set.map(preprocessing_tasks, batched=True)\n","    processed_testset = test_set.map(preprocessing_test_tasks, batched=True)\n","\n","    # Tokenize data and handle the tokenizer added tokens to start and end\n","    # of each sequence.\n","    tokenized_dataset = processed_dataset.map(label_tokenize_alignment,\n","                                              batched=True)\n","    tokenized_testset = processed_testset.map(label_tokenize_alignment,\n","                                              batched=True)\n","\n","    # Define arguments for training.\n","    args = TrainingArguments(\n","        \"test-ner\",\n","        evaluation_strategy = \"epoch\",\n","        learning_rate=MODEL_CONFIG[\"learning_rate\"],\n","        per_device_train_batch_size=MODEL_CONFIG[\"batch_size\"],\n","        per_device_eval_batch_size=MODEL_CONFIG[\"batch_size\"],\n","        num_train_epochs=MODEL_CONFIG[\"num_epochs\"],\n","        weight_decay=MODEL_CONFIG[\"weight_decay\"],\n","    )\n","\n","    # Define the trainer.\n","    trainer = Trainer(\n","        model,\n","        args,\n","        train_dataset=tokenized_dataset[\"train\"],\n","        eval_dataset=tokenized_dataset[\"validation\"],\n","        data_collator=data_collator,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    # Begin Training with train dataset and validation dataset.\n","    trainer.train()\n","\n","    # Evaluate model using test dataset.\n","    test_results = trainer.evaluate(tokenized_testset[\"test\"])\n","    # Output the metrics from the model test.\n","    print(f\"\"\"\\n    Loss: {test_results[\"eval_loss\"]:.4f}\n","    F1 Score: {test_results[\"eval_f1\"]:.4f}\n","    Accuracy: {test_results[\"eval_accuracy\"]:.4f}\\n\"\"\")\n","    print(f\"    Evaluation time taken: {test_results['eval_runtime']:.4f}\")\n","\n","    # Set the model to predict tokens based on the test set.\n","    test_preds = trainer.predict(tokenized_testset[\"test\"])\n","\n","    # Compute metrics as before, except we only care about the predictions\n","    # and the true labels hence we use return_labels = True\n","    metric_results = compute_metrics((test_preds.predictions,\n","                                      tokenized_testset[\"test\"][\"ner_tags\"]),\n","                                      return_labels = True)\n","\n","    # Return the true labels and predicted labels for the confusion matrix\n","    return(\n","        metric_results[\"flattened_predictions\"],\n","        metric_results[\"flattened_true_labels\"]\n","    )"]},{"cell_type":"code","source":["# @title Confusion Matrix generator\n","\n","# Generates a confusion matrix using true labels and predictions from model\n","# evaluation, it then plots it for manual analysis.\n","\n","def confusion_matrix_gen(true_labels, predictions, matrix_title):\n","    conf_matrix = confusion_matrix(true_labels, predictions)\n","\n","    plt.figure(figsize=(5, 3))\n","    sns.heatmap(conf_matrix, annot=True, fmt=\"g\",\n","                cmap=\"Blues\", xticklabels=LABEL_LIST, yticklabels=LABEL_LIST)\n","    plt.xlabel(\"Predicted labels\")\n","    plt.ylabel(\"True labels\")\n","    plt.title(matrix_title)\n","    plt.show()"],"metadata":{"cellView":"form","id":"Ohq_0-BVobU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Configuration Setup\n","\n","# A single function to encapsulate the whole process of running an experiment.\n","# Runs preprocessing, data augmentation, ner_tag mapping, model training,\n","# model evaluation, model testing and confusion matrix plotting.\n","\n","def experiment_variation(matrix_title = \"Confusion Matrix\",\n","                         preprocess = [], augments = [], **kwargs):\n","    # Reset the configs from any previous experiments, simply ensures only the\n","    # chosen settings apply for this run.\n","    reset_configs()\n","    reset_augments()\n","\n","    # Checks for all the model settings parsed into this function.\n","    for param, value in kwargs.items():\n","        MODEL_CONFIG[param] = value\n","\n","    # Checks for all the preprocessing settings parsed into this function.\n","    for task in preprocess:\n","        PREPROCESSING_CONFIG[task] = True\n","\n","    # Checks for all the data augmentation settings parsed into this function.\n","    for task in augments:\n","        AUGMENTATION_CONFIG[task] = True\n","\n","    # Get flattened prediction and true labels from test dataset evaluation,\n","    # also runs the training, validation and evaluation of the model.\n","    flat_pred, flat_true = train_validate_evaluate()\n","\n","    # Generate a confusion matrix using the flattened predictions and labels.\n","    confusion_matrix_gen(flat_true, flat_pred, matrix_title)\n"],"metadata":{"id":"eh3rPhMjdyTY","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Using the experiment_variation function:\n","\n","parameters:\n","\n","        matrix_title   = type:[str]\n","        preprocess     = type:[list[str]]\n","        augments       = type:[list[str]]\n","        learning_rate  = type:[float]\n","        num_epochs     = type:[int]\n","        weight_decay   = type:[float]\n","        batch_size     = type:[int]\n","\n","For preprocess you have thse options, pick as many as you want:\n","\n","        apply_lemmatization\n","        apply_stemming\n","        apply_random_deletion\n","        apply_lowercasing\n","        apply_url_removal\n","        apply_stop_word_removal\n","\n","\n","For augments you have thse options, pick as many as you want:\n","\n","        apply_synonym_replacement\n","        apply_random_insertion\n","        apply_random_swap"],"metadata":{"id":"hVErnYwolpJm"}},{"cell_type":"markdown","source":["Example function usage:\n","        \n","        # Running back to back experiments without resetting environment will result in altered results.\n","        experiment_variation(\n","            matrix_title = \"Experiment X\",\n","            preprocess = [],\n","            augments = [],\n","            learning_rate = 0.00001,\n","            num_epochs = 1,\n","            weight_decay = 0.01,\n","            batch_size = 16\n","        )"],"metadata":{"id":"h82k0zSyICWP"}},{"cell_type":"markdown","source":["# Experiment 1\n","\n","Hyperparameter Testing\n"],"metadata":{"id":"HnOV8P6wysKu"}},{"cell_type":"code","source":["# @title Testing 1 [Num_Epochs = 3]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 1 [Num_Epochs = 3]\",\n","    num_epochs = 3,\n",")"],"metadata":{"id":"W7B2cXq92TdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 2 [Num_Epochs = 5]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 2 [Num_Epochs = 5]\",\n","    num_epochs = 5\n",")"],"metadata":{"id":"8YgQ4FJ0GLth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 3 [Num_Epochs = 7]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 3 [Num_Epochs = 7]\",\n","    num_epochs = 7\n",")"],"metadata":{"id":"VswyjeAgIGUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 4 [Learning_Rate = 0.000005]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 4 [Learning_Rate = 0.000005]\",\n","    learning_rate = 0.000005,\n","    num_epochs = 7\n",")"],"metadata":{"id":"2LI_jOYtI4CE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 5 [Learning_Rate = 0.00001]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 5 [Learning_Rate = 0.00001]\",\n","    learning_rate = 0.00001,\n","    num_epochs = 7\n",")"],"metadata":{"id":"JBAmaeBDIGLu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 6 [Learning_Rate = 0.00002]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 6 [Learning_Rate = 0.00002]\",\n","    learning_rate = 0.00002,\n","    num_epochs = 7\n",")"],"metadata":{"id":"qrbGUsUmIF96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 7 [Batch_Size = 8]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 7 [Batch_Size = 8]\",\n","    batch_size = 8,\n","    learning_rate = 0.00002,\n","    num_epochs = 7\n",")"],"metadata":{"id":"DWWQFUTOI-9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 8 [Batch_Size = 16]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 8 [Batch_Size = 16]\",\n","    batch_size = 16,\n","    learning_rate = 0.00002,\n","    num_epochs = 7\n",")"],"metadata":{"id":"wovm-NP2JChX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 9 [Batch_Size = 32]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 9 [Batch_Size = 32]\",\n","    batch_size = 32,\n","    learning_rate = 0.00002,\n","    num_epochs = 7\n",")"],"metadata":{"id":"Rj0tKC-TJCVT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 10 [Weight_Decay = 0.04]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 10 [Weight_Decay = 0.04]\",\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"tgSknbeQJckv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 11 [Weight_Decay = 0.08]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 11 [Weight_Decay = 0.08]\",\n","    weight_decay = 0.08,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"q1fLq0GYJcag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 12 [Weight_Decay = 0.12]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 1 - Testing 12 [Weight_Decay = 0.12]\",\n","    weight_decay = 0.12,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"dIom7pQieb5J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 2\n","\n","Preprocessing Testing"],"metadata":{"id":"sSC-R7IQ0XPE"}},{"cell_type":"code","source":["# @title Testing 1 [Lemmatization]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 2 - Testing 1 [Lemmatization]\",\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"Zky8OXnHMLko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 2 [Stemming]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 2 - Testing 2 [Stemming]\",\n","    preprocess = [\"apply_stemming\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")\n"],"metadata":{"id":"5nDvmnkRMLiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 3 [Random Deletion]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 2 - Testing 3 [Random Deletion]\",\n","    preprocess = [\"apply_random_deletion\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")\n"],"metadata":{"id":"wHB46IqmMLgK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 4 [Lowercasing]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 2 - Testing 4 [Lowercasing]\",\n","    preprocess = [\"apply_lowercasing\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"XcC3ZTJaMLdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 5 [Url Removal]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 2 - Testing 5 [Url Removal]\",\n","    preprocess = [\"apply_url_removal\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"pYFlxctHMLbY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 6 [Stop Word]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 2 - Testing 6 [Stop Word]\",\n","    preprocess = [\"apply_stop_word_removal\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"1q-Wn5mlMLUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 7 [Random Deletion, Url Removal, Stop Word]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 2 - Testing 7 [Random Deletion, Url Removal, Stop Word]\",\n","    preprocess = [\"apply_random_deletion\", \"apply_url_removal\",\n","                  \"apply_stop_word_removal\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"b1317PyDMgMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 8 [Random Deletion, Url Removal, Stop Word, Lowercasing, Lemmatiziation]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 2 - Testing 8 [Random Deletion, Url Removal, Stop Word, Lowercasing, Lemmatiziation]\",\n","    preprocess = [\"apply_random_deletion\", \"apply_url_removal\",\n","                  \"apply_stop_word_removal\", \"apply_lowercasing\",\n","                  \"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"O_ItEJ5jMgH2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 9 [Random Deletion, Url Removal, Stop Word, Lowercasing, Stemming]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 2 - Testing 9 [Random Deletion, Url Removal, Stop Word, Lowercasing, Stemming]\",\n","    preprocess = [\"apply_random_deletion\", \"apply_url_removal\",\n","                  \"apply_stop_word_removal\", \"apply_lowercasing\",\n","                  \"apply_stemming\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"XlQ-IsI7MgEf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 3\n","\n"],"metadata":{"id":"ZJ00s-yk0ejU"}},{"cell_type":"code","source":["# @title Testing 1 [Synonym Replacement]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 3 - Testing 1 [Synonym Replacement]\",\n","    augments = [\"apply_synonym_replacement\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"0n9SHvr22UN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 2 [Random Insertion]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 3 - Testing 2 [Random Insertion]\",\n","    augments = [\"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"OLYzmztVRhan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 3 [Random Swapping]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 3 - Testing 3 [Random Swapping]\",\n","    augments = [\"apply_random_swap\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"GWJaL5EeRhYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 4 [Synonym Replacement, Random Insertion]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 3 - Testing 4 [Synonym Replacement, Random Insertion]\",\n","    augments = [\"apply_synonym_replacement\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"03f_H5ZmRhVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 5 [Synonym Replacement, Random Swapping]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 3 - Testing 5 [Synonym Replacement, Random Swapping]\",\n","    augments = [\"apply_synonym_replacement\", \"apply_random_swap\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"exIw3s6qRhS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 6 [Random Swapping, Random Insertion]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 3 -Testing 6 [Random Swapping, Random Insertion]\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"CfLiqB4ARhOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 7 [Synonym Replacement, Random Swapping, Random Insertion]\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 3 - Testing 7 [Synonym Replacement, Random Swapping, Random Insertion]\",\n","    augments = [\"apply_synonym_replacement\", \"apply_random_swap\",\n","                \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"KDyH6pqcRhQt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment 4\n"],"metadata":{"id":"-LYx79Ym0exS"}},{"cell_type":"code","source":["# @title Testing 1 - Increasing Num_Epochs to 9\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Increasing Num_Epochs to 9\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 9,\n","    batch_size = 16\n",")"],"metadata":{"id":"Hk-IKNUJ2UGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 2 - Increasing Num_Epochs to 8\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Increasing Num_Epochs to 8\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 8,\n","    batch_size = 16\n",")"],"metadata":{"id":"3IgxlBcffmJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 3 - Decreasing Batch_Size to 8\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Decreasing Batch_Size to 8\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00002,\n","    num_epochs = 7,\n","    batch_size = 8\n",")"],"metadata":{"id":"9bQabBh1fmbv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 4 - Increasing Learning_Rate to 0.00003\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Increasing Learning_Rate to 0.00003\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.00003,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"cAhgvFXOAf7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 5 - Increasing Learning_Rate to 0.000035\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Increasing Learning_Rate to 0.000035\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.000035,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"uySQb_hYDSER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 6 - Increasing Learning_Rate to 0.0000325\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Increasing Learning_Rate to 0.0000325\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.0000325,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"T28czc8-D6Mc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 7 - Increasing Learning_Rate to 0.0000275\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Increasing Learning_Rate to 0.0000275\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.04,\n","    learning_rate = 0.0000275,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"D1zLxbWZEhs3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 8 - Decreasing Weight Decay To 0.03\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Decreasing Weight Decay To 0.03\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.03,\n","    learning_rate = 0.00003,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"9T3B8AKGFRlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 9 - Decreasing Weight Decay To 0.025\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Decreasing Weight Decay To 0.025\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.025,\n","    learning_rate = 0.00003,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"FBkNFtyPGRii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Testing 10 - Decreasing Weight Decay To 0.035\n","\n","experiment_variation(\n","    matrix_title = \"Experiment 4 - Decreasing Weight Decay To 0.035\",\n","    augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","    preprocess = [\"apply_lemmatization\"],\n","    weight_decay = 0.035,\n","    learning_rate = 0.00003,\n","    num_epochs = 7,\n","    batch_size = 16\n",")"],"metadata":{"id":"slZ71ZeUG7k-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Best case parameters:\n","        \n","        experiment_variation(\n","            matrix_title = \" \",\n","            augments = [\"apply_random_swap\", \"apply_random_insertion\"],\n","            preprocess = [\"apply_lemmatization\"],\n","            weight_decay = 0.03,\n","            learning_rate = 0.00003,\n","            num_epochs = 7,\n","            batch_size = 16\n","        )"],"metadata":{"id":"OkQfo9zdI4ez"}}],"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","toc_visible":true,"collapsed_sections":["HnOV8P6wysKu","sSC-R7IQ0XPE"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}